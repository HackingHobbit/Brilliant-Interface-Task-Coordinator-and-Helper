/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
Command: npx gltfjsx@6.2.3 public/models/64f1a714fe61576b46f27ca2.glb -o src/components/Avatar.jsx -k -r public
*/

import { useAnimations, useGLTF } from "@react-three/drei";
import { useFrame } from "@react-three/fiber";
import { button, useControls } from "leva";
import React, { useEffect, useRef, useState } from "react";

import * as THREE from "three";
import { VISEMES } from "wawa-lipsync";
import { useChat } from "../hooks/useChat";
import { useLipsync } from "../hooks/useLipsync";
import { webSpeechTTS } from "../utils/webSpeechTTS";

const facialExpressions = {
  default: {},
  smile: {
    browInnerUp: 0.17,
    eyeSquintLeft: 0.4,
    eyeSquintRight: 0.44,
    noseSneerLeft: 0.1700000727403593,
    noseSneerRight: 0.14000002836874015,
    mouthPressLeft: 0.61,
    mouthPressRight: 0.41000000000000003,
  },
  funnyFace: {
    jawLeft: 0.63,
    mouthPucker: 0.53,
    noseSneerLeft: 1,
    noseSneerRight: 0.39,
    mouthLeft: 1,
    eyeLookUpLeft: 1,
    eyeLookUpRight: 1,
    cheekPuff: 0.9999924982764238,
    mouthDimpleLeft: 0.414743888682652,
    mouthRollLower: 0.32,
    mouthSmileLeft: 0.35499733688813034,
    mouthSmileRight: 0.35499733688813034,
  },
  sad: {
    mouthFrownLeft: 1,
    mouthFrownRight: 1,
    mouthShrugLower: 0.78341,
    browInnerUp: 0.452,
    eyeSquintLeft: 0.72,
    eyeSquintRight: 0.75,
    eyeLookDownLeft: 0.5,
    eyeLookDownRight: 0.5,
    jawForward: 1,
  },
  surprised: {
    eyeWideLeft: 0.5,
    eyeWideRight: 0.5,
    jawOpen: 0.351,
    mouthFunnel: 1,
    browInnerUp: 1,
  },
  angry: {
    browDownLeft: 1,
    browDownRight: 1,
    eyeSquintLeft: 1,
    eyeSquintRight: 1,
    jawForward: 1,
    jawLeft: 1,
    mouthShrugLower: 1,
    noseSneerLeft: 1,
    noseSneerRight: 0.42,
    eyeLookDownLeft: 0.16,
    eyeLookDownRight: 0.16,
    cheekSquintLeft: 1,
    cheekSquintRight: 1,
    mouthClose: 0.23,
    mouthFunnel: 0.63,
    mouthDimpleRight: 1,
  },
  crazy: {
    browInnerUp: 0.9,
    jawForward: 1,
    noseSneerLeft: 0.5700000000000001,
    noseSneerRight: 0.51,
    eyeLookDownLeft: 0.39435766259644545,
    eyeLookUpRight: 0.4039761421719682,
    eyeLookInLeft: 0.9618479575523053,
    eyeLookInRight: 0.9618479575523053,
    jawOpen: 0.9618479575523053,
    mouthDimpleLeft: 0.9618479575523053,
    mouthDimpleRight: 0.9618479575523053,
    mouthStretchLeft: 0.27893590769016857,
    mouthStretchRight: 0.2885543872656917,
    mouthSmileLeft: 0.5578718153803371,
    mouthSmileRight: 0.38473918302092225,
    tongueOut: 0.9618479575523053,
  },
};

const corresponding = {
  A: "viseme_PP",
  B: "viseme_kk",
  C: "viseme_I",
  D: "viseme_aa",  // Fixed: lowercase 'aa' instead of 'AA'
  E: "viseme_O",
  F: "viseme_U",
  G: "viseme_FF",
  H: "viseme_TH",
  X: "viseme_PP",
};

let setupMode = false;

export function Avatar(props) {
  const { nodes, materials, scene } = useGLTF(
    "/models/64f1a714fe61576b46f27ca2.glb"
  );

  // Debug: Log available morph targets once
  useEffect(() => {
    if (nodes.EyeLeft?.morphTargetDictionary) {
      console.log("Available morph targets:", Object.keys(nodes.EyeLeft.morphTargetDictionary));
      console.log("Viseme-related morph targets:",
        Object.keys(nodes.EyeLeft.morphTargetDictionary).filter(key =>
          key.toLowerCase().includes('viseme') || key.toLowerCase().includes('mouth')
        )
      );
    }
  }, [nodes]);

  const { message, onMessagePlayed, chat } = useChat();
  const { connectAudio, startAnalysis, stopAnalysis, currentViseme } = useLipsync();

  const [lipsync, setLipsync] = useState();

  useEffect(() => {
    console.log("=== NEW MESSAGE EFFECT ===");
    console.log("Message received:", message);
    console.log("Currently speaking:", isSpeaking);
    console.log("Current audio exists:", !!audio);

    if (!message) {
      // Only stop audio when there's no message
      if (audio) {
        console.log("Stopping audio - no message");
        audio.pause();
        audio.currentTime = 0;
      }
      webSpeechTTS.stop();
      setAnimation("Idle");
      setFacialExpression("default");
      stopAnalysis();
      setIsSpeaking(false);
      setCurrentMessageId(null);
      return;
    }

    // Create a unique ID for this message to prevent duplicate processing
    const messageId = `${message.text.substring(0, 50)}_${message.animation}_${!!message.audio}`;
    console.log("Message ID:", messageId);
    console.log("Current Message ID:", currentMessageId);

    // If this is the same message we're already processing, ignore it
    if (currentMessageId === messageId) {
      console.log("ðŸš« Ignoring duplicate message processing");
      return;
    }

    // If we're already speaking, stop the current audio first
    if (isSpeaking && audio && !audio.paused) {
      console.log("ðŸ›‘ Stopping current audio to play new message");
      audio.pause();
      audio.currentTime = 0;
      webSpeechTTS.stop();
      stopAnalysis();
      setIsSpeaking(false);
    }

    setCurrentMessageId(messageId);

    // Set animation (fallback to Talking_0 if invalid)
    const validAnimations = ["Talking_0", "Talking_1", "Talking_2", "Crying", "Laughing", "Rumba", "Idle", "Terrified", "Angry"];
    const animation = validAnimations.includes(message.animation) ? message.animation : "Talking_0";
    console.log("Setting animation:", animation);
    setAnimation(animation);

    // Handle special case for winking
    if (message.facialExpression === "wink") {
      console.log("Winking!");
      setFacialExpression("smile"); // Use smile as base expression
      // Trigger a wink
      setWinkLeft(true);
      setTimeout(() => setWinkLeft(false), 300);
    } else {
      // Set facial expression (fallback to default if invalid)
      const validExpressions = ["default", "smile", "sad", "angry", "surprised", "funnyFace", "crazy"];
      const expression = validExpressions.includes(message.facialExpression) ? message.facialExpression : "default";
      console.log("Setting facial expression:", expression, "(original:", message.facialExpression + ")");
      setFacialExpression(expression);
    }

    setLipsync(message.lipsync);

    console.log('Audio handling - message.audio exists:', !!message.audio);
    console.log('Audio data length:', message.audio ? message.audio.length : 0);

    if (message.audio && message.audio.length > 0) {
      console.log('ðŸŽµ Using Piper TTS audio');
      console.log('ðŸŽµ Audio data length:', message.audio.length);

      // Test if the base64 data looks valid
      const audioDataUrl = "data:audio/wav;base64," + message.audio;
      console.log('ðŸŽµ Audio data URL prefix:', audioDataUrl.substring(0, 50));

      // Create audio element and connect to lip sync
      const newAudio = new Audio(audioDataUrl);

      // Add more detailed debugging
      newAudio.onloadstart = () => console.log('ðŸŽµ Audio load started');
      newAudio.oncanplay = () => {
        console.log('ðŸŽµ Audio can play');
        console.log('ðŸŽµ Audio duration:', newAudio.duration);
        console.log('ðŸŽµ Audio ready state:', newAudio.readyState);
      };
      newAudio.onloadeddata = () => console.log('ðŸŽµ Audio data loaded');
      newAudio.onloadedmetadata = () => {
        console.log('ðŸŽµ Audio metadata loaded');
        console.log('ðŸŽµ Duration:', newAudio.duration, 'seconds');
      };

      // Connect audio to lip sync manager
      connectAudio(newAudio);

      newAudio.onplay = () => {
        console.log('ðŸŽµ Piper audio started playing');
        console.log('ðŸŽµ Audio volume:', newAudio.volume);
        console.log('ðŸŽµ Audio muted:', newAudio.muted);
        console.log('ðŸŽµ Audio duration:', newAudio.duration);
        startAnalysis();
        setIsSpeaking(true);
      };

      newAudio.onended = () => {
        console.log('ðŸŽµ Piper audio ended');
        stopAnalysis();
        setIsSpeaking(false);
        setCurrentMessageId(null);
        onMessagePlayed();
      };

      newAudio.onpause = () => {
        console.log('ðŸŽµ Piper audio paused');
        stopAnalysis();
        setIsSpeaking(false);
      };

      newAudio.onerror = (error) => {
        console.error('ðŸŽµ Piper audio error:', error);
        console.error('ðŸŽµ Audio error details:', newAudio.error);
        stopAnalysis();
        setIsSpeaking(false);
        setCurrentMessageId(null);
        onMessagePlayed();
      };

      // Try to play with more debugging
      console.log('ðŸŽµ Attempting to play audio...');
      const playPromise = newAudio.play();

      if (playPromise !== undefined) {
        playPromise
          .then(() => {
            console.log('ðŸŽµ Audio play promise resolved successfully');
          })
          .catch(error => {
            console.error('ðŸŽµ Audio play promise rejected:', error);
            // Fallback to Web Speech API
            console.log('ðŸŽµ Falling back to Web Speech API');
            webSpeechTTS.speak(message.text, {
              onStart: () => {
                console.log('ðŸ—£ï¸ Web Speech TTS started (fallback)');
                setIsSpeaking(true);
              },
              onEnd: () => {
                console.log('ðŸ—£ï¸ Web Speech TTS ended (fallback)');
                setIsSpeaking(false);
                setCurrentMessageId(null);
                onMessagePlayed();
              },
              onError: (error) => {
                console.error('ðŸ—£ï¸ TTS Error (fallback):', error);
                setIsSpeaking(false);
                setCurrentMessageId(null);
                onMessagePlayed();
              }
            });
          });
      }

      setAudio(newAudio);
    } else {
      console.log('ðŸ—£ï¸ Using Web Speech API fallback');
      // Use Web Speech API as fallback (without real-time lip sync for now)
      setAudio(null);

      // Use Web Speech API for speech synthesis
      webSpeechTTS.speak(message.text, {
        onStart: () => {
          console.log('ðŸ—£ï¸ Web Speech TTS started');
          setIsSpeaking(true);
        },
        onEnd: () => {
          console.log('ðŸ—£ï¸ Web Speech TTS ended');
          setIsSpeaking(false);
          setCurrentMessageId(null);
          onMessagePlayed();
        },
        onError: (error) => {
          console.error('ðŸ—£ï¸ TTS Error:', error);
          setIsSpeaking(false);
          setCurrentMessageId(null);
          onMessagePlayed();
        }
      });
    }
  }, [message, connectAudio, startAnalysis, stopAnalysis, onMessagePlayed]);

  const { animations } = useGLTF("/models/animations.glb");

  const group = useRef();
  const { actions, mixer } = useAnimations(animations, group);
  const [animation, setAnimation] = useState(
    animations.find((a) => a.name === "Idle") ? "Idle" : animations[0].name // Check if Idle animation exists otherwise use first animation
  );
  useEffect(() => {
    actions[animation]
      .reset()
      .fadeIn(mixer.stats.actions.inUse === 0 ? 0 : 0.5)
      .play();
    return () => actions[animation].fadeOut(0.5);
  }, [animation]);

  const lerpMorphTarget = (target, value, speed = 0.1) => {
    let targetFound = false;

    scene.traverse((child) => {
      if (child.isSkinnedMesh && child.morphTargetDictionary) {
        const index = child.morphTargetDictionary[target];
        if (
          index === undefined ||
          child.morphTargetInfluences[index] === undefined
        ) {
          return;
        }

        targetFound = true;
        child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
          child.morphTargetInfluences[index],
          value,
          speed
        );

        if (!setupMode) {
          try {
            set({
              [target]: value,
            });
          } catch (e) {}
        }
      }
    });

    // Debug: Log when a viseme morph target is not found
    if (!targetFound && target.toLowerCase().includes('viseme')) {
      console.warn(`Morph target not found in any mesh: ${target}`);
    }

    // Debug: Log when a viseme morph target is activated
    if (targetFound && target.toLowerCase().includes('viseme') && value > 0.5) {
      console.log(`Active viseme: ${target} (value: ${value.toFixed(2)})`);
    }
  };

  const [blink, setBlink] = useState(false);
  const [winkLeft, setWinkLeft] = useState(false);
  const [winkRight, setWinkRight] = useState(false);
  const [facialExpression, setFacialExpression] = useState("");
  const [audio, setAudio] = useState();
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [currentMessageId, setCurrentMessageId] = useState(null);

  useFrame(() => {
    if (setupMode) return;

    // Debug current viseme state
    if (isSpeaking && currentViseme) {
      console.log(`ðŸ” Debug - isSpeaking: ${isSpeaking}, currentViseme: ${currentViseme}`);
    }

    // Handle facial expressions
    Object.keys(nodes.EyeLeft.morphTargetDictionary).forEach((key) => {
      // Skip eye blink which is handled separately
      if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
        return;
      }

      // Get the mapping for the current facial expression
      const mapping = facialExpressions[facialExpression] || facialExpressions["default"];

      if (mapping && mapping[key]) {
        lerpMorphTarget(key, mapping[key], 0.1);
      } else {
        lerpMorphTarget(key, 0, 0.1);
      }
    });

    // Handle eye blinking
    lerpMorphTarget("eyeBlinkLeft", blink || winkLeft ? 1 : 0, 0.5);
    lerpMorphTarget("eyeBlinkRight", blink || winkRight ? 1 : 0, 0.5);

    // REAL-TIME LIPSYNC with wawa-lipsync (when audio is available)
    // For Web Speech API, we'll use a simple talking animation instead
    if (currentViseme && currentViseme !== "viseme_sil") {
      console.log(`ðŸŽ¯ Applying viseme: ${currentViseme}`);
      lerpMorphTarget(currentViseme, 1, 0.2);

      // Reset all other visemes using VISEMES from wawa-lipsync
      Object.values(VISEMES).forEach((value) => {
        if (value !== currentViseme) {
          lerpMorphTarget(value, 0, 0.1);
        }
      });
    } else if (isSpeaking) {
      // Simple talking animation for Web Speech API (when no real-time lip sync)
      // Animate between different mouth shapes during speech
      const time = Date.now() * 0.01;
      const talkingIntensity = Math.sin(time) * 0.3 + 0.3; // Oscillate between 0 and 0.6

      // Use a mix of visemes for natural talking motion
      lerpMorphTarget(VISEMES.PP, Math.sin(time * 1.2) * talkingIntensity, 0.1);
      lerpMorphTarget(VISEMES.aa, Math.sin(time * 0.8 + 1) * talkingIntensity, 0.1);
      lerpMorphTarget(VISEMES.I, Math.sin(time * 1.5 + 2) * talkingIntensity, 0.1);

      // Reset other visemes using VISEMES constant
      Object.values(VISEMES).forEach(viseme => {
        if (viseme !== VISEMES.PP && viseme !== VISEMES.aa && viseme !== VISEMES.I) {
          lerpMorphTarget(viseme, 0, 0.1);
        }
      });
    } else {
      // Reset all visemes when not speaking using VISEMES from wawa-lipsync
      Object.values(VISEMES).forEach((value) => {
        lerpMorphTarget(value, 0, 0.1);
      });
    }
  });

  useControls("FacialExpressions", {
    chat: button(() => chat()),
    winkLeft: button(() => {
      setWinkLeft(true);
      setTimeout(() => setWinkLeft(false), 300);
    }),
    winkRight: button(() => {
      setWinkRight(true);
      setTimeout(() => setWinkRight(false), 300);
    }),
    animation: {
      value: animation,
      options: animations.map((a) => a.name),
      onChange: (value) => setAnimation(value),
    },
    facialExpression: {
      options: Object.keys(facialExpressions),
      onChange: (value) => setFacialExpression(value),
    },
    enableSetupMode: button(() => {
      setupMode = true;
    }),
    disableSetupMode: button(() => {
      setupMode = false;
    }),
    logMorphTargetValues: button(() => {
      const emotionValues = {};
      Object.keys(nodes.EyeLeft.morphTargetDictionary).forEach((key) => {
        if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
          return; // eyes wink/blink are handled separately
        }
        const value =
          nodes.EyeLeft.morphTargetInfluences[
            nodes.EyeLeft.morphTargetDictionary[key]
          ];
        if (value > 0.01) {
          emotionValues[key] = value;
        }
      });
      console.log(JSON.stringify(emotionValues, null, 2));
    }),
  });

  const [, set] = useControls("MorphTarget", () =>
    Object.assign(
      {},
      ...Object.keys(nodes.EyeLeft.morphTargetDictionary).map((key) => {
        return {
          [key]: {
            label: key,
            value: 0,
            min: nodes.EyeLeft.morphTargetInfluences[
              nodes.EyeLeft.morphTargetDictionary[key]
            ],
            max: 1,
            onChange: (val) => {
              if (setupMode) {
                lerpMorphTarget(key, val, 1);
              }
            },
          },
        };
      })
    )
  );

  useEffect(() => {
    let blinkTimeout;
    const nextBlink = () => {
      blinkTimeout = setTimeout(() => {
        setBlink(true);
        setTimeout(() => {
          setBlink(false);
          nextBlink();
        }, 200);
      }, THREE.MathUtils.randInt(1000, 5000));
    };
    nextBlink();
    return () => clearTimeout(blinkTimeout);
  }, []);

  return (
    <group {...props} dispose={null} ref={group}>
      <primitive object={nodes.Hips} />
      <skinnedMesh
        name="Wolf3D_Body"
        geometry={nodes.Wolf3D_Body.geometry}
        material={materials.Wolf3D_Body}
        skeleton={nodes.Wolf3D_Body.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Bottom"
        geometry={nodes.Wolf3D_Outfit_Bottom.geometry}
        material={materials.Wolf3D_Outfit_Bottom}
        skeleton={nodes.Wolf3D_Outfit_Bottom.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Footwear"
        geometry={nodes.Wolf3D_Outfit_Footwear.geometry}
        material={materials.Wolf3D_Outfit_Footwear}
        skeleton={nodes.Wolf3D_Outfit_Footwear.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Top"
        geometry={nodes.Wolf3D_Outfit_Top.geometry}
        material={materials.Wolf3D_Outfit_Top}
        skeleton={nodes.Wolf3D_Outfit_Top.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Hair"
        geometry={nodes.Wolf3D_Hair.geometry}
        material={materials.Wolf3D_Hair}
        skeleton={nodes.Wolf3D_Hair.skeleton}
      />
      <skinnedMesh
        name="EyeLeft"
        geometry={nodes.EyeLeft.geometry}
        material={materials.Wolf3D_Eye}
        skeleton={nodes.EyeLeft.skeleton}
        morphTargetDictionary={nodes.EyeLeft.morphTargetDictionary}
        morphTargetInfluences={nodes.EyeLeft.morphTargetInfluences}
      />
      <skinnedMesh
        name="EyeRight"
        geometry={nodes.EyeRight.geometry}
        material={materials.Wolf3D_Eye}
        skeleton={nodes.EyeRight.skeleton}
        morphTargetDictionary={nodes.EyeRight.morphTargetDictionary}
        morphTargetInfluences={nodes.EyeRight.morphTargetInfluences}
      />
      <skinnedMesh
        name="Wolf3D_Head"
        geometry={nodes.Wolf3D_Head.geometry}
        material={materials.Wolf3D_Skin}
        skeleton={nodes.Wolf3D_Head.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Head.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Head.morphTargetInfluences}
      />
      <skinnedMesh
        name="Wolf3D_Teeth"
        geometry={nodes.Wolf3D_Teeth.geometry}
        material={materials.Wolf3D_Teeth}
        skeleton={nodes.Wolf3D_Teeth.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Teeth.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Teeth.morphTargetInfluences}
      />
    </group>
  );
}

useGLTF.preload("/models/64f1a714fe61576b46f27ca2.glb");
useGLTF.preload("/models/animations.glb");
